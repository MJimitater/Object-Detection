{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "From Kaggle-Competition \"Dogs vs. Cats\", a data set of 12500 dog images and 12500 cat images provided.\n",
    "The goal is to build a classifier, that can predict if either a dog (=1) or a cat (=0) is in the image.\n",
    "For the sake of running at low computational cost, we will only train the model on 1000 dog images and 1000 cat images.\n",
    "Plus, we will only use 400 images of each class to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#This is to import our images to keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#specify, where\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "# set dimensions of images to 100x100\n",
    "img_width, img_height = 100, 100\n",
    "#total training images is 2000, namly 1000 dog images and 1000 cat images:\n",
    "nb_train_samples = 2000\n",
    "#total validation images is 800\n",
    "#the validation images are only to evaluate the model and calculate the loss\n",
    "nb_validation_samples = 800\n",
    "#set epochs to 20: complete data set is run through 20 times\n",
    "#too few epochs means bad accuracy\n",
    "#too many epochs means overfitting and great computational cost\n",
    "epochs = 20\n",
    "#set batch_size to 20: gradients will be upgraded everytime after 20 images\n",
    "batch_size = 20\n",
    "\n",
    "#rescale train images: pixel values are between [0,255], rescale them\n",
    "#between [0,1] for faster convergence\n",
    "train_datagen = ImageDataGenerator( rescale = 1. / 255 )\n",
    "\n",
    "#rescale test images accordingly\n",
    "test_datagen = ImageDataGenerator( rescale = 1. / 255 )\n",
    "\n",
    "#load training images from directory:\n",
    "#subfolders named \"dogs\" and \"cats\" will be taken as labels\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "#validation data we will need to evaluate the loss and the model metrics after each epoch\n",
    "#this data will NOT be trained\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these imports are needed to build our Convolutional Neural Network (CNN)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "#Sequential() will configure the model for training    \n",
    "model = Sequential()\n",
    "\n",
    "#First: Input image (100,100,3) for (width, height, channels)\n",
    "#and connect input layer with 2D convolutional layer (this is needed for images);\n",
    "#choose 32 filters and a kernel size of 3x3.\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width,img_height,3)))\n",
    "#choose rectified linear unit as activation function;\n",
    "#most of the time this is industry standard\n",
    "model.add(Activation('relu'))\n",
    "#perform max pooling\n",
    "#pool size of (2,2) to halve vertical and horizontal dimension of input\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#perform this bundle of convolutions, activation and max pooling again\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#activation and max pooling again\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#flatten layers out\n",
    "model.add(Flatten())\n",
    "#create a dense layer of 32 neurons\n",
    "model.add(Dense(32))\n",
    "#perform relu\n",
    "model.add(Activation('relu'))\n",
    "#perform dropout of 0.5\n",
    "#dropout will randomly swith neurons on and off\n",
    "#with a 0.5 chance to minimize overfitting\n",
    "model.add(Dropout(0.5))\n",
    "#finish off with one output unit..\n",
    "model.add(Dense(1))\n",
    "#..the sigmoid function will act as a logistic function:\n",
    "# a prediction as to 1 will predict its a dog\n",
    "# and a prediction as to 0 will predict its a cat.\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "#lets configure the model for training:\n",
    "#choose binary_crossentropy as loss-function:\n",
    "#we have a binary classification problem\n",
    "#for keras optimizer choose 'rmsprop',\n",
    "#other optimizers e.g. 'adam' work well too\n",
    "#activate metrics so training can be overlooked\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 483s 5s/step - loss: 0.7034 - acc: 0.5255 - val_loss: 0.6835 - val_acc: 0.5075\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.6757 - acc: 0.5790 - val_loss: 0.6769 - val_acc: 0.5500\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 55s 553ms/step - loss: 0.6360 - acc: 0.6315 - val_loss: 0.6619 - val_acc: 0.5788\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 0.6005 - acc: 0.6875 - val_loss: 0.6053 - val_acc: 0.6537\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 56s 558ms/step - loss: 0.5705 - acc: 0.7070 - val_loss: 0.5658 - val_acc: 0.7112\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 0.5416 - acc: 0.7330 - val_loss: 0.5529 - val_acc: 0.7063\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 0.5087 - acc: 0.7565 - val_loss: 0.5393 - val_acc: 0.7350\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 207s 2s/step - loss: 0.4876 - acc: 0.7685 - val_loss: 0.5438 - val_acc: 0.7275\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 55s 553ms/step - loss: 0.4524 - acc: 0.7955 - val_loss: 0.6007 - val_acc: 0.6963\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 0.4193 - acc: 0.8170 - val_loss: 0.5624 - val_acc: 0.7388\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 0.4025 - acc: 0.8160 - val_loss: 0.5632 - val_acc: 0.7350\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 58s 577ms/step - loss: 0.3738 - acc: 0.8370 - val_loss: 0.5707 - val_acc: 0.7475\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 57s 568ms/step - loss: 0.3632 - acc: 0.8350 - val_loss: 0.6590 - val_acc: 0.7237\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.3297 - acc: 0.8600 - val_loss: 0.5782 - val_acc: 0.7388\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.3046 - acc: 0.8710 - val_loss: 0.6062 - val_acc: 0.7337\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 57s 567ms/step - loss: 0.2875 - acc: 0.8795 - val_loss: 0.6485 - val_acc: 0.7313\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 0.2645 - acc: 0.8945 - val_loss: 0.7269 - val_acc: 0.7412\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 0.2360 - acc: 0.9090 - val_loss: 0.7582 - val_acc: 0.7137\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 0.2286 - acc: 0.8990 - val_loss: 0.9133 - val_acc: 0.6988\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 59s 595ms/step - loss: 0.2089 - acc: 0.9120 - val_loss: 0.7567 - val_acc: 0.7375\n"
     ]
    }
   ],
   "source": [
    "#For large data sets like this one, use model.fit_generator\n",
    "model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    #total number of steps between two epochs:\n",
    "    #set to (nb_train_samples / batch_size) and round off\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "#once training is done, save the whole model incl. architecture, weights etc.\n",
    "model.save('CNN_model.h5')\n",
    "\n",
    "#Go to predict.ipynb to use this trained CNN to predict dog vs. cat images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}